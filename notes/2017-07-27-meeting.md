## Meeting notes
### What has been done
#### Extracted dom features and analyzed
Extracted the dom features but noticed that the data from sites is not very well distributed. On further inspection of
structures within the DOM, noticed that site-specific tags such as `comment` appear more frequently as they are used
on the site `okazii`. More data will be needed from the other websites to provide better representation of the situation 
when doing exploratory analysis.

Apart from an unexpected `div > comment` association which indicated bias towards one site, the analysis did reveal expected
structures such as lists could be predicted from the features. 

#### What can be done
1. Labeling the data(the tags)
2. Extracting visual features 
3. Generating XPath/css selectors from predictions


There are a  couple of papers that use visual features in the context of web content analysis(ie. *Web Page Element Classification Based on Visual Features - Radek Burget*) which could provide a basis for the extraction of visual features. My approach intendes to use the box coordinates 
and perhaps generate a image based on them which could be fed to a convolutional network for classification, then fed as a feature.
Also there is *ViDE: A Vision-based Approach for Deep Web Data Extraction - Wei Liu, Xiaofeng Meng*. Both of them use visual features
for clustering or classification of elements on a page with decent results, but none use deep models for this. This might be worth 
investigating.

Labeling the data, can be at first done manually, using tailor-made XPaths for a couple few scraped websites.

**The problem of generalization:** For labeling: In *Cross-Supervised Synthesis of Web-Crawlers - Adi Omari* they use a method called 
cross-supervisation to first manualy input the data for one site, then search for similar nodes(using a defined equivalence 
relationship) on other websites and generating crawlers for them. The algorithm overall is not based on ML but rather on 
a greedy approach.

A two-fold process for scraping may be implemented in which such an equivalence relationship is trained with model and, like in the paper, and then using the so-called *cross-supervisation*, generate labeled data on the new site and train a single standing model there.

Visual features could be able to generalize more to other websites. The approach of incremental dataset building should be
explored as should be the *bottom-up* building of the model from similar content(although it assumes a common denominator
to be present on both the pages - maybe train a content to identify content itself, not within the context of a dom)
Ie. same exact text.

*Task:* 
1. Label the data using manual XPaths for the given websites
2. try to see if a deep model using dom features can
correctly classify the desired nodes. These features may be more robust than XPath against layout changes
3. Generate faster, less computationally-intesive XPath from calssification
4. Try to extract visual features features from the data
5. Further investigate the incremental approach.(*cross-supervision*)
